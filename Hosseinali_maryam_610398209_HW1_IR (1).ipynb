{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1: Information Retrieval System**"
      ],
      "metadata": {
        "id": "BCpiR07Nm86N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UUK-RZAyPnKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoOX4SuknCA0",
        "outputId": "2c43e05f-5d0c-439e-b853-cba0e395a561"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1\n",
        "file_names = ['1.txt', '2.txt', '3.txt']\n",
        "\n",
        "\n",
        "def preprocess_text(file_name):\n",
        "\n",
        "    with open(file_name, 'r') as file:\n",
        "        text = file.read()\n",
        "\n",
        "\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "\n",
        "\n",
        "    tokens = [token for token in tokens if token.isalpha()]\n",
        "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "preprocessed_docs = {}\n",
        "for idx, file_name in enumerate(file_names, 1):\n",
        "    preprocessed_docs[idx] = preprocess_text(file_name)\n",
        "\n",
        "\n",
        "for doc_id, tokens in preprocessed_docs.items():\n",
        "    print(f\"Document {doc_id} tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKhvH35anlM8",
        "outputId": "1e3b110d-dd20-4781-e40c-c26b2b13ab34"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1 tokens: ['simple', 'example', 'document', 'contains', 'several', 'words', 'words', 'processed', 'starting', 'third', 'term', 'main', 'focus', 'research', 'guided', 'advisor', 'supervisory', 'committee']\n",
            "Document 2 tokens: ['another', 'example', 'document', 'different', 'content', 'spelling', 'correction', 'important', 'retrieval', 'time', 'doctoral', 'student', 'allowed', 'participate', 'two', 'summer', 'internships', 'internships', 'subject', 'approval', 'dean', 'graduate', 'studies', 'secured', 'offer', 'must', 'submit', 'summer', 'internship', 'form', 'form', 'must', 'accompanied', 'offer', 'letter', 'description', 'relevancy', 'current', 'studies', 'report', 'due', 'conclusion', 'internship', 'sent', 'graduate', 'school', 'administrative', 'dean', 'office', 'october', 'forms', 'found', 'online', 'address', 'matters', 'like', 'course', 'drop', 'leave', 'absence', 'degree', 'petition', 'students', 'responsible', 'ensuring', 'forms', 'due', 'university', 'registrar', 'given', 'departmental', 'registrar', 'well', 'advance', 'deadline']\n",
            "Document 3 tokens: ['another', 'example', 'document', 'test', 'boolean', 'search', 'capabilities', 'document', 'contains', 'relevant', 'content', 'may', 'known', 'life', 'yet', 'blah', 'blah', 'blah']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Step 1: Document Preprocessing\n",
        "\n",
        "In this step, three text files (`1.txt`, `2.txt`, and `3.txt`) are read, tokenized, and preprocessed.\n",
        "\n",
        "1. **Reading Files**: The `preprocess_text` function reads the content of each file.\n",
        "2. **Tokenization**: The text is converted to lowercase and split into individual words (tokens) using `nltk.word_tokenize()`.\n",
        "3. **Removing Punctuation and Stopwords**: Non-alphabetic characters and common stopwords are removed to focus on meaningful words.\n",
        "4. **Storing Results**: The preprocessed tokens are stored in the `preprocessed_docs` dictionary for each document.\n"
      ],
      "metadata": {
        "id": "4IYkai69oh-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 2\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def get_vocab(docs):\n",
        "    vocab = set()\n",
        "    for words in docs.values():\n",
        "        vocab.update(words)\n",
        "    return sorted(vocab)\n",
        "\n",
        "def get_index(docs, vocab):\n",
        "    index = defaultdict(list)\n",
        "    for term in vocab:\n",
        "        for doc, words in docs.items():\n",
        "            if term in words:\n",
        "                index[term].append(doc)\n",
        "    return index\n",
        "\n",
        "\n",
        "vocab = get_vocab(preprocessed_docs)\n",
        "index = get_index(preprocessed_docs, vocab)\n",
        "\n",
        "\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"\\nInverted Index:\")\n",
        "for term, doc_list in index.items():\n",
        "    print(f\"{term}: {doc_list}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Dddqi6I0H_h",
        "outputId": "fe1a8da3-22ba-4bd0-f60f-de0fef2c8f05"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['absence', 'accompanied', 'address', 'administrative', 'advance', 'advisor', 'allowed', 'another', 'approval', 'blah', 'boolean', 'capabilities', 'committee', 'conclusion', 'contains', 'content', 'correction', 'course', 'current', 'deadline', 'dean', 'degree', 'departmental', 'description', 'different', 'doctoral', 'document', 'drop', 'due', 'ensuring', 'example', 'focus', 'form', 'forms', 'found', 'given', 'graduate', 'guided', 'important', 'internship', 'internships', 'known', 'leave', 'letter', 'life', 'like', 'main', 'matters', 'may', 'must', 'october', 'offer', 'office', 'online', 'participate', 'petition', 'processed', 'registrar', 'relevancy', 'relevant', 'report', 'research', 'responsible', 'retrieval', 'school', 'search', 'secured', 'sent', 'several', 'simple', 'spelling', 'starting', 'student', 'students', 'studies', 'subject', 'submit', 'summer', 'supervisory', 'term', 'test', 'third', 'time', 'two', 'university', 'well', 'words', 'yet']\n",
            "\n",
            "Inverted Index:\n",
            "absence: [2]\n",
            "accompanied: [2]\n",
            "address: [2]\n",
            "administrative: [2]\n",
            "advance: [2]\n",
            "advisor: [1]\n",
            "allowed: [2]\n",
            "another: [2, 3]\n",
            "approval: [2]\n",
            "blah: [3]\n",
            "boolean: [3]\n",
            "capabilities: [3]\n",
            "committee: [1]\n",
            "conclusion: [2]\n",
            "contains: [1, 3]\n",
            "content: [2, 3]\n",
            "correction: [2]\n",
            "course: [2]\n",
            "current: [2]\n",
            "deadline: [2]\n",
            "dean: [2]\n",
            "degree: [2]\n",
            "departmental: [2]\n",
            "description: [2]\n",
            "different: [2]\n",
            "doctoral: [2]\n",
            "document: [1, 2, 3]\n",
            "drop: [2]\n",
            "due: [2]\n",
            "ensuring: [2]\n",
            "example: [1, 2, 3]\n",
            "focus: [1]\n",
            "form: [2]\n",
            "forms: [2]\n",
            "found: [2]\n",
            "given: [2]\n",
            "graduate: [2]\n",
            "guided: [1]\n",
            "important: [2]\n",
            "internship: [2]\n",
            "internships: [2]\n",
            "known: [3]\n",
            "leave: [2]\n",
            "letter: [2]\n",
            "life: [3]\n",
            "like: [2]\n",
            "main: [1]\n",
            "matters: [2]\n",
            "may: [3]\n",
            "must: [2]\n",
            "october: [2]\n",
            "offer: [2]\n",
            "office: [2]\n",
            "online: [2]\n",
            "participate: [2]\n",
            "petition: [2]\n",
            "processed: [1]\n",
            "registrar: [2]\n",
            "relevancy: [2]\n",
            "relevant: [3]\n",
            "report: [2]\n",
            "research: [1]\n",
            "responsible: [2]\n",
            "retrieval: [2]\n",
            "school: [2]\n",
            "search: [3]\n",
            "secured: [2]\n",
            "sent: [2]\n",
            "several: [1]\n",
            "simple: [1]\n",
            "spelling: [2]\n",
            "starting: [1]\n",
            "student: [2]\n",
            "students: [2]\n",
            "studies: [2]\n",
            "subject: [2]\n",
            "submit: [2]\n",
            "summer: [2]\n",
            "supervisory: [1]\n",
            "term: [1]\n",
            "test: [3]\n",
            "third: [1]\n",
            "time: [2]\n",
            "two: [2]\n",
            "university: [2]\n",
            "well: [2]\n",
            "words: [1]\n",
            "yet: [3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Step 2: Building the Vocabulary and Inverted Index\n",
        "\n",
        "In this step, we create a **vocabulary** and an **inverted index** from the preprocessed documents.\n",
        "\n",
        "1. **Vocabulary Creation**:  \n",
        "   The function `get_vocab` generates a sorted list of all unique words (tokens) from the preprocessed documents. These words form the **vocabulary** of the dataset.\n",
        "\n",
        "2. **Inverted Index Creation**:  \n",
        "   The function `get_index` builds an inverted index, which maps each term in the vocabulary to a list of document IDs where that term appears.\n",
        "\n",
        "3. **Output**:  \n",
        "   We print the vocabulary and the inverted index to view all terms and the documents they are associated with.\n",
        "\n"
      ],
      "metadata": {
        "id": "9Ke0rAAKpXHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 3\n",
        "from nltk.metrics import edit_distance\n",
        "\n",
        "\n",
        "with open('Typos.txt', 'r') as f:\n",
        "    typos = f.read().splitlines()\n",
        "\n",
        "\n",
        "def get_two_nearest_words(word, vocab):\n",
        "    distances = [(v, edit_distance(word, v)) for v in vocab]\n",
        "\n",
        "    sorted_words = sorted(distances, key=lambda x: (x[1], x[0]))\n",
        "    return [w for w, _ in sorted_words[:2]]\n",
        "\n",
        "\n",
        "for line in typos:\n",
        "\n",
        "    words = line.split()\n",
        "\n",
        "\n",
        "    words = [word for word in words if word not in ['and', 'or', 'not', 'near'] and not word.startswith('near/')]\n",
        "\n",
        "    for word in words:\n",
        "        word = word.strip()\n",
        "        if word:\n",
        "            suggestions = get_two_nearest_words(word, vocab)\n",
        "\n",
        "            print(f\"{word}: {suggestions}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gZhxgEAAFfT",
        "outputId": "7e60877f-f7b3-40c8-8da0-a192f9dbdd6c"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "expmple: ['example', 'simple']\n",
            "tst: ['test', 'must']\n",
            "university: ['university', 'different']\n",
            "min: ['main', 'may']\n",
            "blh: ['blah', 'due']\n",
            "found: ['found', 'focus']\n",
            "contnt: ['content', 'contains']\n",
            "ofer: ['offer', 'due']\n",
            "examplee: ['example', 'simple']\n",
            "start: ['search', 'sent']\n",
            "simple: ['simple', 'example']\n",
            "tme: ['time', 'due']\n",
            "scure: ['secured', 'course']\n",
            "bla: ['blah', 'dean']\n",
            "expmple: ['example', 'simple']\n",
            "test: ['test', 'must']\n",
            "exomple: ['example', 'simple']\n",
            "seach: ['search', 'blah']\n",
            "admnstratve: ['administrative', 'advance']\n",
            "cnotent: ['content', 'current']\n",
            "onlne: ['online', 'blah']\n",
            "blh: ['blah', 'due']\n",
            "process: ['processed', 'address']\n",
            "rtwo: ['two', 'blah']\n",
            "another: ['another', 'letter']\n",
            "leve: ['leave', 'life']\n",
            "booln: ['boolean', 'known']\n",
            "univrst: ['university', 'course']\n",
            "hulk: ['due', 'must']\n",
            "smash: ['blah', 'main']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Step 3: Spell Checking Using Edit Distance\n",
        "\n",
        "In this step, we process the misspelled words from the file `Typos.txt` and generate two suggested corrections for each word based on the **edit distance** between the misspelled word and the words in our vocabulary.\n",
        "\n",
        "1. **Reading Typos**:  \n",
        "   The file `Typos.txt` is read and split into individual lines, each containing a query with potential misspellings.\n",
        "\n",
        "2. **Finding Nearest Words**:  \n",
        "   The function `get_two_nearest_words` calculates the **edit distance** between each misspelled word and the words in the vocabulary. It returns the two nearest words (based on the smallest edit distance).\n",
        "\n",
        "3. **Handling Logical Notations**:  \n",
        "   I filterd logical terms like `\"and\"`, `\"or\"`, `\"not\"`, and any `\"near/number\"` patterns, ensuring that only the actual misspelled words are processed.\n",
        "\n",
        "4. **Output**:  \n",
        "   For each misspelled word, the code prints the two closest suggestions from the vocabulary based on edit distance.\n",
        "\n",
        "**Note**:  \n",
        "In the 9th test case, `\"exomple near/4 seach\"`, I was not sure how to handle the logical notation `\"near/4\"`, so I ignored the term `\"near/4\"` and processed only the words `\"exomple\"` and `\"seach\"`.\n",
        "\n"
      ],
      "metadata": {
        "id": "LzbfKzxNTx1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 4\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def create_bigram_index(vocabulary):\n",
        "    bigram_index = defaultdict(set)\n",
        "\n",
        "    for word in vocabulary:\n",
        "\n",
        "        bigrams = [word[i:i+2] for i in range(len(word) - 1)]\n",
        "        for bigram in bigrams:\n",
        "            bigram_index[bigram].add(word)\n",
        "\n",
        "    return bigram_index\n",
        "\n",
        "\n",
        "def handle_wildcard(query, bigram_index):\n",
        "\n",
        "    query_parts = query.split('*')\n",
        "\n",
        "\n",
        "    bigrams = []\n",
        "    for part in query_parts:\n",
        "        if len(part) > 1:\n",
        "            bigrams.extend([part[i:i+2] for i in range(len(part) - 1)])\n",
        "\n",
        "\n",
        "    if bigrams:\n",
        "\n",
        "        candidates = bigram_index[bigrams[0]].copy() if bigrams[0] in bigram_index else set()\n",
        "\n",
        "\n",
        "        for bigram in bigrams[1:]:\n",
        "            if bigram in bigram_index:\n",
        "                candidates &= bigram_index[bigram]\n",
        "            else:\n",
        "                return []\n",
        "\n",
        "    else:\n",
        "        candidates = set(vocab)\n",
        "\n",
        "\n",
        "    matches = [word for word in candidates if match_wildcard(word, query_parts)]\n",
        "\n",
        "    return matches\n",
        "\n",
        "\n",
        "def match_wildcard(word, query_parts):\n",
        "    start_idx = 0\n",
        "    for i, part in enumerate(query_parts):\n",
        "        if part:\n",
        "            idx = word.find(part, start_idx)\n",
        "            if idx == -1 or (i == 0 and idx != 0):\n",
        "                return False\n",
        "            start_idx = idx + len(part)\n",
        "\n",
        "\n",
        "    if query_parts[-1] and not word.endswith(query_parts[-1]):\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "bigram_index = create_bigram_index(vocab)\n",
        "\n",
        "\n",
        "with open('Wildcards.txt', 'r') as f:\n",
        "    wildcard_queries = f.readlines()\n",
        "\n",
        "\n",
        "generic_terms = {'and', 'or', 'not', 'near'}\n",
        "\n",
        "\n",
        "wildcard_results = {}\n",
        "for query in wildcard_queries:\n",
        "    query = query.strip()\n",
        "    if not query:\n",
        "        continue\n",
        "    words_in_query = query.split()\n",
        "\n",
        "\n",
        "    words_in_query = [word for word in words_in_query if word not in generic_terms and not word.startswith('near/')]\n",
        "\n",
        "    results = []\n",
        "    for word in words_in_query:\n",
        "        if '*' in word:\n",
        "            matches = handle_wildcard(word, bigram_index)\n",
        "            results.append(f\"{word} : {matches if matches else '[]'}\")\n",
        "        else:\n",
        "            results.append(f\"{word} : {word}\")\n",
        "    wildcard_results[query] = results\n",
        "\n",
        "\n",
        "print(\"Refined Wildcard Query Results:\")\n",
        "for query, matches in wildcard_results.items():\n",
        "    for match in matches:\n",
        "        print(match)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ8EL1iM1wK5",
        "outputId": "74425c4c-babc-4431-e02a-9cb2d8204e3b"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refined Wildcard Query Results:\n",
            "we*l : ['well']\n",
            "t*me : ['time']\n",
            "dea* : ['dean', 'deadline']\n",
            "on*i*e : ['online']\n",
            "dea*li*e : ['deadline']\n",
            "con*ent : ['content']\n",
            "abs*nc* : ['absence']\n",
            "rel*v*nt : ['relevant']\n",
            "st*dent : ['student']\n",
            "gi**n : ['given']\n",
            "retrie*l : ['retrieval']\n",
            "cont*nt : ['content']\n",
            "exmp*e : []\n",
            "y*t : ['yet']\n",
            "doct*r*l : ['doctoral']\n",
            "allow* : ['allowed']\n",
            "sum**r : ['summer']\n",
            "sup*rvi*ory : ['supervisory']\n",
            "third : third\n",
            "ma*n : ['main']\n",
            "scho* : ['school']\n",
            "ex*pl* : ['example']\n",
            "se* : ['search', 'several', 'sent', 'secured']\n",
            "su*er : ['summer']\n",
            "adm*n*strative : ['administrative']\n",
            "c*nta*n : []\n",
            "un*vers*ty : ['university']\n",
            "resp*sible : ['responsible']\n",
            "bl*h : ['blah']\n",
            "ens*r*ng : ['ensuring']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Step 4: Handling Wildcard Queries with Bi-gram Indexing\n",
        "\n",
        "In this step, we handle wildcard queries from `Wildcards.txt` using **bi-gram indexing** to efficiently find potential matches in the vocabulary.\n",
        "\n",
        "1. **Bi-gram Index Creation**:  \n",
        "   The function `create_bigram_index` builds a bi-gram index by breaking each word in the vocabulary into overlapping bi-grams. This allows us to search for wildcard patterns by comparing bi-grams.\n",
        "\n",
        "2. **Handling Wildcards**:  \n",
        "   The function `handle_wildcard` processes wildcard queries by splitting them at the `*` symbol, generating bi-grams from each part of the query, and then finding words in the vocabulary that match these bi-grams. The function `match_wildcard` ensures the candidates conform to the original wildcard pattern.\n",
        "\n",
        "3. **Refined Results**:  \n",
        "   For each wildcard query, logical operators such as `\"and\"`, `\"or\"`, `\"not\"`, and patterns like `\"near/number\"` are removed before matching. The results for each query show the first match from the vocabulary or an empty list if no match is found.\n",
        "\n",
        "**Note 1**:  \n",
        "In the wildcard query `\"c*nta*n\"`, I decided not to match it with `\"contains\"` because it does not logically fit. The wildcard `\"c*nta*n\"` suggests missing letters are replaced by `*`, but `\"contains\"` has an additional \"s\" at the end, which is not accounted for in the pattern.\n",
        "\n",
        "**Note 2**:  \n",
        "In the 8th test case `\"doct*r*l near/3 allow*\"`, I ignored the term `\"near/3\"` since I was unsure of how to interpret the logical notation. I processed only the words `\"doct*r*l\"` and `\"allow*\"`.\n",
        "\n"
      ],
      "metadata": {
        "id": "0rEQ_gUAmo2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 5 - part Typos\n",
        "import nltk\n",
        "from nltk.metrics import edit_distance\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "docs = {\n",
        "    1: ['simple', 'example', 'document', 'contains', 'several', 'words', 'processed', 'starting', 'third', 'term',\n",
        "        'main', 'focus', 'research', 'guided', 'advisor', 'supervisory', 'committee'],\n",
        "    2: ['another', 'example', 'document', 'different', 'content', 'spelling', 'correction', 'important', 'retrieval',\n",
        "        'time', 'doctoral', 'student', 'allowed', 'participate', 'two', 'summer', 'internships', 'subject', 'approval',\n",
        "        'dean', 'graduate', 'studies', 'secured', 'offer', 'must', 'submit', 'internship', 'form', 'accompanied',\n",
        "        'letter', 'description', 'relevancy', 'current', 'report', 'due', 'conclusion', 'administrative', 'office',\n",
        "        'october', 'forms', 'found', 'online', 'address', 'course', 'drop', 'leave', 'absence', 'degree', 'petition',\n",
        "        'students', 'responsible', 'ensuring', 'university', 'registrar', 'given', 'departmental', 'well', 'advance',\n",
        "        'deadline'],\n",
        "    3: ['another', 'example', 'document', 'test', 'boolean', 'search', 'capabilities', 'contains', 'relevant', 'content',\n",
        "        'may', 'known', 'life', 'yet', 'blah', 'blah', 'blah']\n",
        "}\n",
        "\n",
        "\n",
        "def create_inverted_index(docs):\n",
        "    index = defaultdict(list)\n",
        "    for doc_id, tokens in docs.items():\n",
        "        for word in tokens:\n",
        "            index[word].append(doc_id)\n",
        "    return index\n",
        "\n",
        "\n",
        "inverted_index = create_inverted_index(docs)\n",
        "\n",
        "vocab = set(word for words in docs.values() for word in words)\n",
        "\n",
        "\n",
        "def get_two_nearest_words(word, vocab):\n",
        "    distances = [(v, edit_distance(word, v)) for v in vocab]\n",
        "\n",
        "    sorted_words = sorted(distances, key=lambda x: (x[1], x[0]))\n",
        "    return [w for w, _ in sorted_words[:2]]\n",
        "\n",
        "\n",
        "with open('Typos.txt', 'r') as f:\n",
        "    typos = f.read().splitlines()\n",
        "\n",
        "\n",
        "first_suggestions = {}\n",
        "for line in typos:\n",
        "\n",
        "    words = line.split()\n",
        "    words = [word for word in words if word not in ['and', 'or', 'not', 'near'] and not word.startswith('near/')]\n",
        "\n",
        "    for word in words:\n",
        "        word = word.strip()\n",
        "        if word:\n",
        "\n",
        "            suggestions = get_two_nearest_words(word, vocab)\n",
        "\n",
        "            first_suggestions[word] = suggestions[0]\n",
        "\n",
        "\n",
        "def retrieve_documents_for_query(query, first_suggestions, inverted_index):\n",
        "    words_in_query = query.split()\n",
        "    result_docs = set()\n",
        "    current_docs = set()\n",
        "    operation = None\n",
        "    for word in words_in_query:\n",
        "        word = word.strip()\n",
        "\n",
        "        if word == \"and\":\n",
        "            operation = \"and\"\n",
        "        elif word == \"or\":\n",
        "            operation = \"or\"\n",
        "        elif word == \"not\":\n",
        "            operation = \"not\"\n",
        "        else:\n",
        "\n",
        "            corrected_word = first_suggestions.get(word, word)\n",
        "\n",
        "\n",
        "            if corrected_word in inverted_index:\n",
        "                current_docs = set(inverted_index[corrected_word])\n",
        "            else:\n",
        "                current_docs = set()\n",
        "\n",
        "\n",
        "            if operation == \"and\":\n",
        "                result_docs &= current_docs\n",
        "            elif operation == \"or\":\n",
        "                result_docs |= current_docs\n",
        "            elif operation == \"not\":\n",
        "                result_docs -= current_docs\n",
        "            else:\n",
        "                result_docs = current_docs\n",
        "            operation = None\n",
        "\n",
        "    return sorted(list(result_docs))\n",
        "\n",
        "\n",
        "original_queries = [\n",
        "    \"expmple or tst\",\n",
        "    \"university and min\",\n",
        "    \"blh not found\",\n",
        "    \"contnt and ofer\",\n",
        "    \"examplee and start\",\n",
        "    \"simple or tme\",\n",
        "    \"scure not bla\",\n",
        "    \"expmple or test\",\n",
        "    \"exomple near/4 seach\",\n",
        "    \"admnstratve or cnotent\",\n",
        "    \"onlne or blh\",\n",
        "    \"process and rtwo\",\n",
        "    \"another or leve\",\n",
        "    \"booln and univrst\",\n",
        "    \"hulk and smash\"\n",
        "]\n",
        "\n",
        "\n",
        "for query in original_queries:\n",
        "    result_docs = retrieve_documents_for_query(query, first_suggestions, inverted_index)\n",
        "    print(f\" {query}: {result_docs}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaDKO-plEwSQ",
        "outputId": "6aae31d8-6263-47a6-d695-5f2c6e63fbd8"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " expmple or tst: [1, 2, 3]\n",
            " university and min: []\n",
            " blh not found: [3]\n",
            " contnt and ofer: [2]\n",
            " examplee and start: [3]\n",
            " simple or tme: [1, 2]\n",
            " scure not bla: [2]\n",
            " expmple or test: [1, 2, 3]\n",
            " exomple near/4 seach: [3]\n",
            " admnstratve or cnotent: [2, 3]\n",
            " onlne or blh: [2, 3]\n",
            " process and rtwo: []\n",
            " another or leve: [2, 3]\n",
            " booln and univrst: []\n",
            " hulk and smash: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Note**:  \n",
        "Fortunately, all results for typos matched perfectly with the provided **typos answers**!\n",
        "\n"
      ],
      "metadata": {
        "id": "kqRdKCQaXzRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 5 - wildcards part\n",
        "import re\n",
        "def handle_wildcard_query(pattern, vocab):\n",
        "\n",
        "    regex_pattern = pattern.replace('*', '.*')\n",
        "    regex = re.compile(f'^{regex_pattern}$')\n",
        "\n",
        "    matches = [word for word in vocab if regex.match(word)]\n",
        "    return matches\n",
        "\n",
        "first_wildcard_matches = {}\n",
        "for line in wildcards:\n",
        "    words = line.split()\n",
        "    words = [word for word in words if word not in ['and', 'or', 'not', 'near'] and not word.startswith('near/')]\n",
        "\n",
        "    for word in words:\n",
        "        word = word.strip()\n",
        "        if '*' in word:\n",
        "            matches = handle_wildcard_query(word, vocab)\n",
        "            if matches:\n",
        "                first_wildcard_matches[word] = matches[0]\n",
        "            else:\n",
        "                first_wildcard_matches[word] = word\n",
        "        else:\n",
        "            first_wildcard_matches[word] = word\n",
        "\n",
        "\n",
        "def retrieve_documents_for_query(query, first_wildcard_matches, inverted_index):\n",
        "    words_in_query = query.split()\n",
        "    result_docs = set()\n",
        "    current_docs = set()\n",
        "    operation = None\n",
        "\n",
        "    for word in words_in_query:\n",
        "        word = word.strip()\n",
        "\n",
        "        if word == \"and\":\n",
        "            operation = \"and\"\n",
        "        elif word == \"or\":\n",
        "            operation = \"or\"\n",
        "        elif word == \"not\":\n",
        "            operation = \"not\"\n",
        "        else:\n",
        "\n",
        "            corrected_word = first_wildcard_matches.get(word, word)\n",
        "\n",
        "\n",
        "            if corrected_word in inverted_index:\n",
        "                current_docs = set(inverted_index[corrected_word])\n",
        "            else:\n",
        "                current_docs = set()\n",
        "\n",
        "\n",
        "            if operation == \"and\":\n",
        "                result_docs &= current_docs\n",
        "            elif operation == \"or\":\n",
        "                result_docs |= current_docs\n",
        "            elif operation == \"not\":\n",
        "                result_docs -= current_docs\n",
        "            else:\n",
        "                result_docs = current_docs\n",
        "            operation = None\n",
        "\n",
        "    return sorted(list(result_docs))\n",
        "\n",
        "\n",
        "wildcard_queries = [\n",
        "    \"we*l and t*me\",\n",
        "    \"dea* or on*i*e\",\n",
        "    \"dea*li*e and con*ent\",\n",
        "    \"abs*nc* or rel*v*nt\",\n",
        "    \"st*dent and gi**n\",\n",
        "    \"retrie*l not cont*nt\",\n",
        "    \"exmp*e not y*t\",\n",
        "    \"doct*r*l near/3 allow*\",\n",
        "    \"sum**r or sup*rvi*ory\",\n",
        "    \"third and ma*n\",\n",
        "    \"scho* and ex*pl*\",\n",
        "    \"se* or su*er\",\n",
        "    \"adm*n*strative or c*nta*n\",\n",
        "    \"un*vers*ty not resp*sible\",\n",
        "    \"bl*h or ens*r*ng\"\n",
        "]\n",
        "\n",
        "\n",
        "for query in wildcard_queries:\n",
        "    result_docs = retrieve_documents_for_query(query, first_wildcard_matches, inverted_index)\n",
        "    print(f\" {query}: {result_docs}\")\n"
      ],
      "metadata": {
        "id": "Y8G5KxV7GKB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4dadbeb-3bfd-402b-e560-53993bd1f18a"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " we*l and t*me: [2]\n",
            " dea* or on*i*e: [2]\n",
            " dea*li*e and con*ent: [2]\n",
            " abs*nc* or rel*v*nt: [2, 3]\n",
            " st*dent and gi**n: [2]\n",
            " retrie*l not cont*nt: []\n",
            " exmp*e not y*t: []\n",
            " doct*r*l near/3 allow*: [2]\n",
            " sum**r or sup*rvi*ory: [1, 2]\n",
            " third and ma*n: [1]\n",
            " scho* and ex*pl*: []\n",
            " se* or su*er: [2, 3]\n",
            " adm*n*strative or c*nta*n: [2]\n",
            " un*vers*ty not resp*sible: []\n",
            " bl*h or ens*r*ng: [2, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Note**:  \n",
        "While most of the results matched the **wildcards answers**, the results for test cases **7, 11, 12, and 13** did not match. However, I believe the answers for test cases **7** and **12** in the provided wildcards answers may be incorrect.\n"
      ],
      "metadata": {
        "id": "4J89yCae4Hjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Step 5: Information Retrieval System for Typos and Wildcards\n",
        "\n",
        "\n",
        "\n",
        "#### **Typos Processing**\n",
        "1. **Edit Distance for Correction**:  \n",
        "   The system reads `Typos.txt` and uses **edit distance** to find the nearest words in the vocabulary. The first suggestion is used for document retrieval.\n",
        "   \n",
        "2. **Logical Operators**:  \n",
        "   The system handles logical operators like `\"and\"`, `\"or\"`, and `\"not\"` to combine results. For example, `\"expmple or tst\"` retrieves documents with `\"example\"` or `\"test\"`.\n",
        "\n",
        "3. **Results**:  \n",
        "   All results matched the provided **typos answers**, confirming the correctness of the typo handling process.\n",
        "\n",
        "#### **Wildcard Processing**\n",
        "1. **Wildcard Matching**:  \n",
        "   Wildcard queries are handled using **bi-gram indexing**. The system matches wildcard patterns with words in the vocabulary and retrieves the relevant documents.\n",
        "\n",
        "2. **Logical Operators**:  \n",
        "   Similar to typos, the system applies logical operators to wildcard queries, combining the results from matched terms.\n",
        "\n",
        "3. **Results**:  \n",
        "   Most results matched the **wildcards answers**, but test cases **7, 11, 12, and 13** did not. I believe the answers for **7** and **12** may be incorrect.\n",
        "\n",
        "#### **Combined System**\n",
        "- The system handles both **typos** and **wildcards**, applying logical operators to retrieve relevant documents.\n",
        "- Terms like `\"near/number\"` were ignored due to ambiguity in interpretation.\n",
        "\n"
      ],
      "metadata": {
        "id": "XqK1EoN9bOFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2: Soundex Encoding**"
      ],
      "metadata": {
        "id": "SBTwocnkdGyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def soundex(name):\n",
        "\n",
        "    name = name.upper()\n",
        "    first_letter = name[0]\n",
        "\n",
        "\n",
        "    remaining_name = ''.join([char for char in name[1:] if char not in 'AEIOUYHW'])\n",
        "\n",
        "\n",
        "    soundex_mapping = {\n",
        "        'B': '1', 'F': '1', 'P': '1', 'V': '1',\n",
        "        'C': '2', 'G': '2', 'J': '2', 'K': '2', 'Q': '2', 'S': '2', 'X': '2', 'Z': '2',\n",
        "        'D': '3', 'T': '3',\n",
        "        'L': '4',\n",
        "        'M': '5', 'N': '5',\n",
        "        'R': '6'\n",
        "    }\n",
        "\n",
        "\n",
        "    encoded_name = first_letter\n",
        "\n",
        "\n",
        "    encoded_name += ''.join([soundex_mapping[char] for char in remaining_name if char in soundex_mapping])\n",
        "\n",
        "\n",
        "    encoded_name = first_letter + ''.join([encoded_name[i] for i in range(1, len(encoded_name)) if encoded_name[i] != encoded_name[i-1]])\n",
        "\n",
        "\n",
        "    if first_letter in soundex_mapping and encoded_name[1] == soundex_mapping[first_letter]:\n",
        "        encoded_name = first_letter + encoded_name[2:]\n",
        "\n",
        "\n",
        "    soundex_code = (encoded_name[:4] + \"000\")[:4]\n",
        "\n",
        "    return soundex_code\n",
        "\n",
        "\n",
        "name1 = \"Sepehr\"\n",
        "soundex_code1 = soundex(name1)\n",
        "print(f\"Soundex code for {name1}: {soundex_code1}\")\n",
        "\n",
        "\n",
        "name2 = \"Maryam\"\n",
        "soundex_code2 = soundex(name2)\n",
        "print(f\"Soundex code for {name2}: {soundex_code2}\")\n"
      ],
      "metadata": {
        "id": "UB0KrgfG34A4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d429809-a71a-4179-9497-5b2ace40aa2a"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Soundex code for Sepehr: S160\n",
            "Soundex code for Maryam: M650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this question, we implemented the **Soundex algorithm** to compute the Soundex code for given names. The Soundex algorithm is a phonetic algorithm used to convert words into codes based on their pronunciation. Below is a summary of how the code works:\n",
        "\n",
        "#### **Soundex Algorithm Breakdown**:\n",
        "\n",
        "1. **Convert Name to Uppercase**:  \n",
        "   The input name is first converted to uppercase for uniformity, and the first letter is retained.\n",
        "\n",
        "2. **Remove Vowels and Certain Letters**:  \n",
        "   The remaining letters (excluding the first letter) are filtered to remove vowels (`A, E, I, O, U`) and ignored characters (`Y, H, W`).\n",
        "\n",
        "3. **Mapping Consonants to Digits**:  \n",
        "   Each consonant is mapped to a corresponding digit using the **Soundex mapping table**:\n",
        "   - `B, F, P, V` → `1`\n",
        "   - `C, G, J, K, Q, S, X, Z` → `2`\n",
        "   - `D, T` → `3`\n",
        "   - `L` → `4`\n",
        "   - `M, N` → `5`\n",
        "   - `R` → `6`\n",
        "\n",
        "4. **Remove Consecutive Duplicate Digits**:  \n",
        "   The code removes consecutive duplicate digits.\n",
        "\n",
        "5. **Handle First Letter's Mapping**:  \n",
        "   If the first letter maps to the same digit as the first consonant, the digit is removed to avoid redundancy.\n",
        "\n",
        "6. **Ensure Code is Four Characters Long**:  \n",
        "   The final Soundex code is padded with zeros or truncated to ensure it is exactly four characters long.\n",
        "\n"
      ],
      "metadata": {
        "id": "ba0ywpMLeNfh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UgtWQbXuJJxj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}