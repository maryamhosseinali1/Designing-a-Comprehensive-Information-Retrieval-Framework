# -*- coding: utf-8 -*-
"""Hosseinali-maryam-610398209-HW1-IR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xSYjGj8f8eNK5dCSrhfn7wkesmn0BAuW

# **Question 1: Information Retrieval System**
"""

import nltk
import string
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords

# step 1
file_names = ['1.txt', '2.txt', '3.txt']


def preprocess_text(file_name):

    with open(file_name, 'r') as file:
        text = file.read()


    tokens = nltk.word_tokenize(text.lower())


    tokens = [token for token in tokens if token.isalpha()]
    tokens = [token for token in tokens if token not in stopwords.words('english')]

    return tokens


preprocessed_docs = {}
for idx, file_name in enumerate(file_names, 1):
    preprocessed_docs[idx] = preprocess_text(file_name)


for doc_id, tokens in preprocessed_docs.items():
    print(f"Document {doc_id} tokens:", tokens)

"""

### Step 1: Document Preprocessing

In this step, three text files (`1.txt`, `2.txt`, and `3.txt`) are read, tokenized, and preprocessed.

1. **Reading Files**: The `preprocess_text` function reads the content of each file.
2. **Tokenization**: The text is converted to lowercase and split into individual words (tokens) using `nltk.word_tokenize()`.
3. **Removing Punctuation and Stopwords**: Non-alphabetic characters and common stopwords are removed to focus on meaningful words.
4. **Storing Results**: The preprocessed tokens are stored in the `preprocessed_docs` dictionary for each document.
"""

# step 2
from collections import defaultdict


def get_vocab(docs):
    vocab = set()
    for words in docs.values():
        vocab.update(words)
    return sorted(vocab)

def get_index(docs, vocab):
    index = defaultdict(list)
    for term in vocab:
        for doc, words in docs.items():
            if term in words:
                index[term].append(doc)
    return index


vocab = get_vocab(preprocessed_docs)
index = get_index(preprocessed_docs, vocab)


print("Vocabulary:", vocab)
print("\nInverted Index:")
for term, doc_list in index.items():
    print(f"{term}: {doc_list}")

"""

### Step 2: Building the Vocabulary and Inverted Index

In this step, we create a **vocabulary** and an **inverted index** from the preprocessed documents.

1. **Vocabulary Creation**:  
   The function `get_vocab` generates a sorted list of all unique words (tokens) from the preprocessed documents. These words form the **vocabulary** of the dataset.

2. **Inverted Index Creation**:  
   The function `get_index` builds an inverted index, which maps each term in the vocabulary to a list of document IDs where that term appears.

3. **Output**:  
   We print the vocabulary and the inverted index to view all terms and the documents they are associated with.

"""

# step 3
from nltk.metrics import edit_distance


with open('Typos.txt', 'r') as f:
    typos = f.read().splitlines()


def get_two_nearest_words(word, vocab):
    distances = [(v, edit_distance(word, v)) for v in vocab]

    sorted_words = sorted(distances, key=lambda x: (x[1], x[0]))
    return [w for w, _ in sorted_words[:2]]


for line in typos:

    words = line.split()


    words = [word for word in words if word not in ['and', 'or', 'not', 'near'] and not word.startswith('near/')]

    for word in words:
        word = word.strip()
        if word:
            suggestions = get_two_nearest_words(word, vocab)

            print(f"{word}: {suggestions}")

"""

### Step 3: Spell Checking Using Edit Distance

In this step, we process the misspelled words from the file `Typos.txt` and generate two suggested corrections for each word based on the **edit distance** between the misspelled word and the words in our vocabulary.

1. **Reading Typos**:  
   The file `Typos.txt` is read and split into individual lines, each containing a query with potential misspellings.

2. **Finding Nearest Words**:  
   The function `get_two_nearest_words` calculates the **edit distance** between each misspelled word and the words in the vocabulary. It returns the two nearest words (based on the smallest edit distance).

3. **Handling Logical Notations**:  
   I filterd logical terms like `"and"`, `"or"`, `"not"`, and any `"near/number"` patterns, ensuring that only the actual misspelled words are processed.

4. **Output**:  
   For each misspelled word, the code prints the two closest suggestions from the vocabulary based on edit distance.

**Note**:  
In the 9th test case, `"exomple near/4 seach"`, I was not sure how to handle the logical notation `"near/4"`, so I ignored the term `"near/4"` and processed only the words `"exomple"` and `"seach"`.

"""

# step 4
from collections import defaultdict


def create_bigram_index(vocabulary):
    bigram_index = defaultdict(set)

    for word in vocabulary:

        bigrams = [word[i:i+2] for i in range(len(word) - 1)]
        for bigram in bigrams:
            bigram_index[bigram].add(word)

    return bigram_index


def handle_wildcard(query, bigram_index):

    query_parts = query.split('*')


    bigrams = []
    for part in query_parts:
        if len(part) > 1:
            bigrams.extend([part[i:i+2] for i in range(len(part) - 1)])


    if bigrams:

        candidates = bigram_index[bigrams[0]].copy() if bigrams[0] in bigram_index else set()


        for bigram in bigrams[1:]:
            if bigram in bigram_index:
                candidates &= bigram_index[bigram]
            else:
                return []

    else:
        candidates = set(vocab)


    matches = [word for word in candidates if match_wildcard(word, query_parts)]

    return matches


def match_wildcard(word, query_parts):
    start_idx = 0
    for i, part in enumerate(query_parts):
        if part:
            idx = word.find(part, start_idx)
            if idx == -1 or (i == 0 and idx != 0):
                return False
            start_idx = idx + len(part)


    if query_parts[-1] and not word.endswith(query_parts[-1]):
        return False

    return True


bigram_index = create_bigram_index(vocab)


with open('Wildcards.txt', 'r') as f:
    wildcard_queries = f.readlines()


generic_terms = {'and', 'or', 'not', 'near'}


wildcard_results = {}
for query in wildcard_queries:
    query = query.strip()
    if not query:
        continue
    words_in_query = query.split()


    words_in_query = [word for word in words_in_query if word not in generic_terms and not word.startswith('near/')]

    results = []
    for word in words_in_query:
        if '*' in word:
            matches = handle_wildcard(word, bigram_index)
            results.append(f"{word} : {matches if matches else '[]'}")
        else:
            results.append(f"{word} : {word}")
    wildcard_results[query] = results


print("Refined Wildcard Query Results:")
for query, matches in wildcard_results.items():
    for match in matches:
        print(match)

"""

### Step 4: Handling Wildcard Queries with Bi-gram Indexing

In this step, we handle wildcard queries from `Wildcards.txt` using **bi-gram indexing** to efficiently find potential matches in the vocabulary.

1. **Bi-gram Index Creation**:  
   The function `create_bigram_index` builds a bi-gram index by breaking each word in the vocabulary into overlapping bi-grams. This allows us to search for wildcard patterns by comparing bi-grams.

2. **Handling Wildcards**:  
   The function `handle_wildcard` processes wildcard queries by splitting them at the `*` symbol, generating bi-grams from each part of the query, and then finding words in the vocabulary that match these bi-grams. The function `match_wildcard` ensures the candidates conform to the original wildcard pattern.

3. **Refined Results**:  
   For each wildcard query, logical operators such as `"and"`, `"or"`, `"not"`, and patterns like `"near/number"` are removed before matching. The results for each query show the first match from the vocabulary or an empty list if no match is found.

**Note 1**:  
In the wildcard query `"c*nta*n"`, I decided not to match it with `"contains"` because it does not logically fit. The wildcard `"c*nta*n"` suggests missing letters are replaced by `*`, but `"contains"` has an additional "s" at the end, which is not accounted for in the pattern.

**Note 2**:  
In the 8th test case `"doct*r*l near/3 allow*"`, I ignored the term `"near/3"` since I was unsure of how to interpret the logical notation. I processed only the words `"doct*r*l"` and `"allow*"`.

"""

# step 5 - part Typos
import nltk
from nltk.metrics import edit_distance
from collections import defaultdict


docs = {
    1: ['simple', 'example', 'document', 'contains', 'several', 'words', 'processed', 'starting', 'third', 'term',
        'main', 'focus', 'research', 'guided', 'advisor', 'supervisory', 'committee'],
    2: ['another', 'example', 'document', 'different', 'content', 'spelling', 'correction', 'important', 'retrieval',
        'time', 'doctoral', 'student', 'allowed', 'participate', 'two', 'summer', 'internships', 'subject', 'approval',
        'dean', 'graduate', 'studies', 'secured', 'offer', 'must', 'submit', 'internship', 'form', 'accompanied',
        'letter', 'description', 'relevancy', 'current', 'report', 'due', 'conclusion', 'administrative', 'office',
        'october', 'forms', 'found', 'online', 'address', 'course', 'drop', 'leave', 'absence', 'degree', 'petition',
        'students', 'responsible', 'ensuring', 'university', 'registrar', 'given', 'departmental', 'well', 'advance',
        'deadline'],
    3: ['another', 'example', 'document', 'test', 'boolean', 'search', 'capabilities', 'contains', 'relevant', 'content',
        'may', 'known', 'life', 'yet', 'blah', 'blah', 'blah']
}


def create_inverted_index(docs):
    index = defaultdict(list)
    for doc_id, tokens in docs.items():
        for word in tokens:
            index[word].append(doc_id)
    return index


inverted_index = create_inverted_index(docs)

vocab = set(word for words in docs.values() for word in words)


def get_two_nearest_words(word, vocab):
    distances = [(v, edit_distance(word, v)) for v in vocab]

    sorted_words = sorted(distances, key=lambda x: (x[1], x[0]))
    return [w for w, _ in sorted_words[:2]]


with open('Typos.txt', 'r') as f:
    typos = f.read().splitlines()


first_suggestions = {}
for line in typos:

    words = line.split()
    words = [word for word in words if word not in ['and', 'or', 'not', 'near'] and not word.startswith('near/')]

    for word in words:
        word = word.strip()
        if word:

            suggestions = get_two_nearest_words(word, vocab)

            first_suggestions[word] = suggestions[0]


def retrieve_documents_for_query(query, first_suggestions, inverted_index):
    words_in_query = query.split()
    result_docs = set()
    current_docs = set()
    operation = None
    for word in words_in_query:
        word = word.strip()

        if word == "and":
            operation = "and"
        elif word == "or":
            operation = "or"
        elif word == "not":
            operation = "not"
        else:

            corrected_word = first_suggestions.get(word, word)


            if corrected_word in inverted_index:
                current_docs = set(inverted_index[corrected_word])
            else:
                current_docs = set()


            if operation == "and":
                result_docs &= current_docs
            elif operation == "or":
                result_docs |= current_docs
            elif operation == "not":
                result_docs -= current_docs
            else:
                result_docs = current_docs
            operation = None

    return sorted(list(result_docs))


original_queries = [
    "expmple or tst",
    "university and min",
    "blh not found",
    "contnt and ofer",
    "examplee and start",
    "simple or tme",
    "scure not bla",
    "expmple or test",
    "exomple near/4 seach",
    "admnstratve or cnotent",
    "onlne or blh",
    "process and rtwo",
    "another or leve",
    "booln and univrst",
    "hulk and smash"
]


for query in original_queries:
    result_docs = retrieve_documents_for_query(query, first_suggestions, inverted_index)
    print(f" {query}: {result_docs}")

"""

**Note**:  
Fortunately, all results for typos matched perfectly with the provided **typos answers**!

"""

# step 5 - wildcards part
import re
def handle_wildcard_query(pattern, vocab):

    regex_pattern = pattern.replace('*', '.*')
    regex = re.compile(f'^{regex_pattern}$')

    matches = [word for word in vocab if regex.match(word)]
    return matches

first_wildcard_matches = {}
for line in wildcards:
    words = line.split()
    words = [word for word in words if word not in ['and', 'or', 'not', 'near'] and not word.startswith('near/')]

    for word in words:
        word = word.strip()
        if '*' in word:
            matches = handle_wildcard_query(word, vocab)
            if matches:
                first_wildcard_matches[word] = matches[0]
            else:
                first_wildcard_matches[word] = word
        else:
            first_wildcard_matches[word] = word


def retrieve_documents_for_query(query, first_wildcard_matches, inverted_index):
    words_in_query = query.split()
    result_docs = set()
    current_docs = set()
    operation = None

    for word in words_in_query:
        word = word.strip()

        if word == "and":
            operation = "and"
        elif word == "or":
            operation = "or"
        elif word == "not":
            operation = "not"
        else:

            corrected_word = first_wildcard_matches.get(word, word)


            if corrected_word in inverted_index:
                current_docs = set(inverted_index[corrected_word])
            else:
                current_docs = set()


            if operation == "and":
                result_docs &= current_docs
            elif operation == "or":
                result_docs |= current_docs
            elif operation == "not":
                result_docs -= current_docs
            else:
                result_docs = current_docs
            operation = None

    return sorted(list(result_docs))


wildcard_queries = [
    "we*l and t*me",
    "dea* or on*i*e",
    "dea*li*e and con*ent",
    "abs*nc* or rel*v*nt",
    "st*dent and gi**n",
    "retrie*l not cont*nt",
    "exmp*e not y*t",
    "doct*r*l near/3 allow*",
    "sum**r or sup*rvi*ory",
    "third and ma*n",
    "scho* and ex*pl*",
    "se* or su*er",
    "adm*n*strative or c*nta*n",
    "un*vers*ty not resp*sible",
    "bl*h or ens*r*ng"
]


for query in wildcard_queries:
    result_docs = retrieve_documents_for_query(query, first_wildcard_matches, inverted_index)
    print(f" {query}: {result_docs}")

"""**Note**:  
While most of the results matched the **wildcards answers**, the results for test cases **7, 11, 12, and 13** did not match. However, I believe the answers for test cases **7** and **12** in the provided wildcards answers may be incorrect.

### Step 5: Information Retrieval System for Typos and Wildcards



#### **Typos Processing**
1. **Edit Distance for Correction**:  
   The system reads `Typos.txt` and uses **edit distance** to find the nearest words in the vocabulary. The first suggestion is used for document retrieval.
   
2. **Logical Operators**:  
   The system handles logical operators like `"and"`, `"or"`, and `"not"` to combine results. For example, `"expmple or tst"` retrieves documents with `"example"` or `"test"`.

3. **Results**:  
   All results matched the provided **typos answers**, confirming the correctness of the typo handling process.

#### **Wildcard Processing**
1. **Wildcard Matching**:  
   Wildcard queries are handled using **bi-gram indexing**. The system matches wildcard patterns with words in the vocabulary and retrieves the relevant documents.

2. **Logical Operators**:  
   Similar to typos, the system applies logical operators to wildcard queries, combining the results from matched terms.

3. **Results**:  
   Most results matched the **wildcards answers**, but test cases **7, 11, 12, and 13** did not. I believe the answers for **7** and **12** may be incorrect.

#### **Combined System**
- The system handles both **typos** and **wildcards**, applying logical operators to retrieve relevant documents.
- Terms like `"near/number"` were ignored due to ambiguity in interpretation.

# **Question 2: Soundex Encoding**
"""

def soundex(name):

    name = name.upper()
    first_letter = name[0]


    remaining_name = ''.join([char for char in name[1:] if char not in 'AEIOUYHW'])


    soundex_mapping = {
        'B': '1', 'F': '1', 'P': '1', 'V': '1',
        'C': '2', 'G': '2', 'J': '2', 'K': '2', 'Q': '2', 'S': '2', 'X': '2', 'Z': '2',
        'D': '3', 'T': '3',
        'L': '4',
        'M': '5', 'N': '5',
        'R': '6'
    }


    encoded_name = first_letter


    encoded_name += ''.join([soundex_mapping[char] for char in remaining_name if char in soundex_mapping])


    encoded_name = first_letter + ''.join([encoded_name[i] for i in range(1, len(encoded_name)) if encoded_name[i] != encoded_name[i-1]])


    if first_letter in soundex_mapping and encoded_name[1] == soundex_mapping[first_letter]:
        encoded_name = first_letter + encoded_name[2:]


    soundex_code = (encoded_name[:4] + "000")[:4]

    return soundex_code


name1 = "Sepehr"
soundex_code1 = soundex(name1)
print(f"Soundex code for {name1}: {soundex_code1}")


name2 = "Maryam"
soundex_code2 = soundex(name2)
print(f"Soundex code for {name2}: {soundex_code2}")

"""
In this question, we implemented the **Soundex algorithm** to compute the Soundex code for given names. The Soundex algorithm is a phonetic algorithm used to convert words into codes based on their pronunciation. Below is a summary of how the code works:

#### **Soundex Algorithm Breakdown**:

1. **Convert Name to Uppercase**:  
   The input name is first converted to uppercase for uniformity, and the first letter is retained.

2. **Remove Vowels and Certain Letters**:  
   The remaining letters (excluding the first letter) are filtered to remove vowels (`A, E, I, O, U`) and ignored characters (`Y, H, W`).

3. **Mapping Consonants to Digits**:  
   Each consonant is mapped to a corresponding digit using the **Soundex mapping table**:
   - `B, F, P, V` → `1`
   - `C, G, J, K, Q, S, X, Z` → `2`
   - `D, T` → `3`
   - `L` → `4`
   - `M, N` → `5`
   - `R` → `6`

4. **Remove Consecutive Duplicate Digits**:  
   The code removes consecutive duplicate digits.

5. **Handle First Letter's Mapping**:  
   If the first letter maps to the same digit as the first consonant, the digit is removed to avoid redundancy.

6. **Ensure Code is Four Characters Long**:  
   The final Soundex code is padded with zeros or truncated to ensure it is exactly four characters long.

"""

